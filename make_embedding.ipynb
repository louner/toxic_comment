{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from gensim.models.keyedvectors import KeyedVectors\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "#from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "word2vec_model_filepath = '/home/louner/school/ml/word2vec-GoogleNews-vectors/GoogleNews-vectors-negative300.bin'\n",
    "vocab_file_path = './data/vocab'\n",
    "word_id_file_path = '%s.json'%(vocab_file_path)\n",
    "batch_size = 5\n",
    "LEAST_WORD_COUNT = 10\n",
    "vocab_size = 20000\n",
    "#w2v = KeyedVectors.load_word2vec_format(word2vec_model_filepath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    sentences_preprocessed = []\n",
    "    for sentence in sentences:\n",
    "        if type(sentence) != str:\n",
    "            continue\n",
    "        sentences_preprocessed.append(sentence.lower())\n",
    "    return sentences_preprocessed\n",
    "\n",
    "def tokenize(sentence):\n",
    "    #for tok in word_tokenize(sentence):\n",
    "    for tok in sentence.split(' '):\n",
    "        yield tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "sentences = df['comment_text'].values.tolist()\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "sentences += df['comment_text'].values.tolist()\n",
    "\n",
    "sentences = preprocess(sentences)\n",
    "\n",
    "vocabs = Counter([tok for sentence in sentences for tok in tokenize(sentence)])\n",
    "with open(vocab_file_path, 'w') as f:\n",
    "    json.dump(vocabs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72587, 50), 72587)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_fpath = 'glove/glove.6B.50d.txt'\n",
    "def read_glove(embed_fpath):\n",
    "    embed_mat = []\n",
    "    word_id = {}\n",
    "    with open(embed_fpath) as f:\n",
    "        for line in f:\n",
    "            toks = line.strip('\\n').split(' ')\n",
    "            idd = len(word_id)\n",
    "            word_id[toks[0]] = idd\n",
    "            embed_mat.append(toks[1:])\n",
    "    embed_mat = np.array(embed_mat)\n",
    "    return word_id, embed_mat\n",
    "\n",
    "word_id, embed_mat = read_glove(embed_fpath)\n",
    "\n",
    "dictionary = {}\n",
    "embed_matrix = []\n",
    "for word in vocabs:\n",
    "    if word in word_id:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        embed_matrix.append(embed_mat[word_id[word]])\n",
    "\n",
    "unk = 'unk'\n",
    "embed_matrix.append(embed_mat[word_id[unk]])\n",
    "dictionary[unk] = len(dictionary)\n",
    "embed_matrix = np.array(embed_matrix)\n",
    "embed_matrix.shape, len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.FileHandler('./log/embeddding.log', mode='w')\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "dictionary = {'@ZERO@': 0, '@UNSEEN@': 1}\n",
    "embed_matrix = np.random.normal(size=(1, embedding_size)).tolist()\n",
    "#embed_matrix = [[0]*embedding_size]\n",
    "\n",
    "with open(vocab_file_path) as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = dict(Counter(vocab).most_common(vocab_size))\n",
    "    \n",
    "absent_words = []\n",
    "for word, count in vocab.items():\n",
    "        try:\n",
    "            vec = w2v.word_vec(word)\n",
    "            embed_matrix.append(vec)\n",
    "            dictionary[word] = len(dictionary)\n",
    "        except:\n",
    "            absent_words.append(word)\n",
    "            logger.error('UNKNOWN %s'%(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16717, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix = np.array(embed_matrix)\n",
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3284"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(absent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = embed_matrix.mean(axis=0), embed_matrix.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3284, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absent_matrix = np.random.normal(mean, std, (len(absent_words), embedding_size))\n",
    "absent_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = embed_matrix.shape[0]\n",
    "for i, word in enumerate(absent_words):\n",
    "    dictionary[word] = i+1+vocab_size\n",
    "embed_matrix = np.concatenate((embed_matrix, absent_matrix))\n",
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_matrix = np.random.normal(size=(len(dictionary), embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable(name='W', shape=embed_matrix.shape, initializer=tf.constant_initializer(embed_matrix))\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(var_list=[W])\n",
    "    saver.save(sess, './models/embed_matrix.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(word_id_file_path, 'w') as f:\n",
    "    json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
